{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in pytorch\n",
    "\n",
    "Just like we did before for q-learning, this time we'll design a lasagne network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS = 'floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa9f4f33470>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits. Keep it simple: CartPole isn't worth deep architectures.\n",
    "agent = nn.Sequential()\n",
    "agent.add_module(\"linear1\",nn.Linear(state_dim,200))\n",
    "agent.add_module(\"relu\", nn.ReLU())\n",
    "agent.add_module(\"linear2\",nn.Linear(200,n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "agent.to(device)\n",
    "agent.device = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(agent,states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    s = torch.tensor(states, dtype=torch.float32, device=agent.device) \n",
    "    out = agent.forward(s)\n",
    "    out = nn.functional.softmax(out, dim=1)\n",
    "    out = out.detach().cpu().numpy()\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_proba(agent,test_states)\n",
    "assert isinstance(\n",
    "    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (\n",
    "    test_states.shape[0], n_actions), \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1),\n",
    "                   1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions andrewards\n",
    "    \"\"\"\n",
    "\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    \n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probas = predict_proba(agent,np.array([s]))[0]\n",
    "\n",
    "        a = np.random.choice(env.action_space.n, p=action_probas)\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    G = []\n",
    "    G_t = 0\n",
    "    for r in reversed(rewards):\n",
    "        G_t  = r + gamma*G_t\n",
    "        G.append(G_t)\n",
    "    return list(reversed(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
    "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Policy gradient is:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta} V^{\\pi} (s_0) = \\int_s d(s) \\int_a \\frac{\\partial \\pi(a|s)}{\\partial \\theta} Q^{\\pi}(s,a) \\ dads  =  \\int_s d(s) \\int_a \\pi(a|s) \\frac{\\partial \\log{\\pi(a|s)}}{\\partial \\theta} Q^{\\pi}(s,a) \\ dads ,$$\n",
    "\n",
    "where \n",
    "\n",
    "$$ d(s) = \\sum^{\\infty}_{t=0} \\gamma^t p(s_t=s|s_0,\\pi) $$\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "    opt = torch.optim.Adam(agent.parameters())\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32, device=agent.device)\n",
    "    cumulative_returns = get_cumulative_rewards(rewards, gamma)\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32, device=agent.device)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = agent.forward(states)\n",
    "    probas = nn.functional.softmax(logits, dim=1)\n",
    "    logprobas = nn.functional.log_softmax(logits, dim=1)\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probas, logprobas]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_proba function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    logprobas_for_actions = logprobas[range(len(actions)),actions]\n",
    "\n",
    "    # REINFORCE objective function\n",
    "    J_hat = torch.mean(logprobas_for_actions*cumulative_returns)\n",
    "\n",
    "    # regularize with entropy\n",
    "    entropy_reg = -(probas*logprobas).sum(-1).mean()\n",
    "\n",
    "    loss = - J_hat - 0.1 * entropy_reg\n",
    "\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:31.330\n",
      "mean reward:58.660\n",
      "mean reward:54.190\n",
      "mean reward:72.870\n",
      "mean reward:88.670\n",
      "mean reward:60.190\n",
      "mean reward:119.460\n",
      "mean reward:174.820\n",
      "mean reward:158.250\n",
      "mean reward:99.430\n",
      "mean reward:106.500\n",
      "mean reward:110.860\n",
      "mean reward:105.610\n",
      "mean reward:117.790\n",
      "mean reward:102.350\n",
      "mean reward:102.500\n",
      "mean reward:122.900\n",
      "mean reward:113.930\n",
      "mean reward:124.820\n",
      "mean reward:149.470\n",
      "mean reward:200.600\n",
      "mean reward:605.380\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "\n",
    "    rewards = [train_on_session(*generate_session())\n",
    "               for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.2807.video000008.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
